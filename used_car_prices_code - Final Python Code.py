# -*- coding: utf-8 -*-
"""Used Car Prices Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n8MoOUfsXL2W6K0hSctFVWrBes_J1Qu8
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import xgboost as xgb
import shap
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

# Set random seed for reproducibility
np.random.seed(42)

# 1. Data Loading
df = pd.read_csv('used_cars_prices.csv', skipinitialspace=True, on_bad_lines='skip')

# Define remove_outliers function
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Clean column names
df.columns = df.columns.str.strip()

# Clean numeric columns by removing $, ", and commas
def clean_numeric(col):
    if col.dtype == 'object':
        col = col.str.replace(r'[\$ ",]', '', regex=True)
        col = pd.to_numeric(col, errors='coerce')
    return col

numeric_cols = ['mileage', 'model_year', 'engine_size', 'price', 'location_encoded']
for col in numeric_cols:
    df[col] = clean_numeric(df[col])

binary_cols = ['fuel_type_diesel', 'fuel_type_petrol', 'transmission_auto']
for col in binary_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)

# Clean categorical columns
for col in ['brand', 'model']:
    df[col] = df[col].str.strip().str.lower().fillna('unknown')

# Handle missing values in numeric columns
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())

# Drop rows with NaN in price
df = df.dropna(subset=['price'])

# Remove outliers for selected columns
for col in ['mileage', 'model_year', 'engine_size', 'price']:
    df = remove_outliers(df, col)

# Features and target
X = df.drop('price', axis=1)
y = df['price']

# Preprocessing pipeline
numeric_features = ['mileage', 'model_year', 'engine_size', 'location_encoded', 'fuel_type_diesel', 'fuel_type_petrol', 'transmission_auto']
categorical_features = ['brand', 'model']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_features)
    ])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# XGBoost model
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=6,
    random_state=42
)

# Pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', xgb_model)
])

# Fit the model
pipeline.fit(X_train, y_train)

# Save the model for Power BI
with open('xgb_model.pkl', 'wb') as f:
    pickle.dump(pipeline, f)

# Model evaluation
y_pred = pipeline.predict(X_test)

r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
print(f"R²: {r2:.2f}")
print(f"RMSE: {rmse:.2f} euros")
print(f"MAE: {mae:.2f} euros")

# Cross-validation
cv_scores_r2 = cross_val_score(pipeline, X, y, cv=5, scoring='r2')
cv_scores_rmse = cross_val_score(pipeline, X, y, cv=5, scoring='neg_root_mean_squared_error')
print(f"Cross-validated R²: {np.mean(cv_scores_r2):.2f} ± {np.std(cv_scores_r2):.2f}")
print(f"Cross-validated RMSE: {-np.mean(cv_scores_rmse):.2f} ± {np.std(cv_scores_rmse):.2f} euros")

# SHAP Explanations
X_train_transformed = preprocessor.transform(X_train)
X_test_transformed = preprocessor.transform(X_test)

explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_test_transformed)

# Feature names
cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()
feature_names = numeric_features + cat_feature_names

# SHAP summary plot
shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names, show=False)
plt.savefig('shap_summary_plot.png')
plt.close()

# SHAP dependence plot for mileage (index 0 in numeric_features)
shap.dependence_plot('mileage', shap_values, X_test_transformed, feature_names=feature_names, show=False)
plt.savefig('shap_mileage_dependence.png')
plt.close()

# SHAP force plot for the first instance
shap.force_plot(explainer.expected_value, shap_values[0], X_test_transformed[0], feature_names=feature_names, matplotlib=True, show=False)
plt.savefig('shap_force_plot.png')
plt.close()

# Additional Plots
# Training loss curve
dtrain = xgb.DMatrix(X_train_transformed, label=y_train)
evals_result = {}
xgb.train(
    params={'eta': 0.1, 'max_depth': 6, 'objective': 'reg:squarederror'},
    dtrain=dtrain,
    num_boost_round=200,
    evals=[(dtrain, 'train')],
    evals_result=evals_result
)

plt.figure()
plt.plot(evals_result['train']['rmse'], label='Training RMSE')
plt.xlabel('Iterations')
plt.ylabel('RMSE')
plt.title('Training Loss Curve')
plt.legend()
plt.savefig('training_loss_curve.png')
plt.close()

# Error distribution histogram
errors = y_test - y_pred
plt.figure()
sns.histplot(errors, bins=50, kde=True)
plt.xlabel('Prediction Error (€)')
plt.title('Error Distribution Histogram')
plt.savefig('error_distribution_histogram.png')
plt.close()

# Feature correlation matrix
correlation_matrix = df[numeric_features + ['price']].corr()
plt.figure()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Feature Correlation Matrix')
plt.savefig('feature_correlation_matrix.png')
plt.close()

print("Plots saved: shap_summary_plot.png, shap_mileage_dependence.png, shap_force_plot.png, training_loss_curve.png, error_distribution_histogram.png, feature_correlation_matrix.png")
print("Data shape after cleaning:", df.shape)
print("Sample data:\n", df.head().to_string())